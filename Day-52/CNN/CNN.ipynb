{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32e5d5f2-85e7-4f63-aea1-cd6636ceffaf",
   "metadata": {},
   "source": [
    "# Convolution Neural Networks (CNNs)\n",
    "\n",
    "Convolutional Neural Network or CNN, is a deep learning algorithm that is widely used for image and video recognition, as well as other tasks involving structured grid data. CNNs are designed to automatically and hierarchically learn complex features from raw input data without manual feature engineering. They have revolutionized the field of computer vision and achieved state-of-the-art performance on various image-related tasks.\n",
    "\n",
    "Here's a simplified explanation of how CNNs work:\n",
    "\n",
    "![CNN](assets/cnn.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0fc85e-9373-48cd-8d1a-9b621fc79e00",
   "metadata": {},
   "source": [
    "## Convolution Layer\n",
    "\n",
    "The input to a CNN is an image or a patch of an image. The first layer in a CNN is a convolutional layer that applies a set of learnable `filters` (also known as `kernels`) to the input image. Each filter performs a convolution operation by sliding across the image, computing dot products between the filter weights and the corresponding pixels in the input. This operation captures local patterns and features, such as edges, corners, and textures.\n",
    "\n",
    "### Kernels aka Filters\n",
    "\n",
    "Let's imagine you have a paragraph of text that you want to understand. When you read the paragraph, you start from the left and move to the right, extracting meaning from the words and sentences. You don't focus on individual letters, but rather interpret groups of letters as words. In a similar way, CNNs use kernels (also known as filters or feature detectors) to understand images.\n",
    "\n",
    "An image is made up of pixels, just like text is made up of letters. Each pixel in an image is associated with a value ranging from 0 to 255, determining its intensity (Or 3 values per pixel for colored images, one for each of red, green and blue channels). The intelligence of CNNs lies in these kernels because they capture the essential information from the image, allowing the network to make predictions or perform tasks like image classification, object detection, or image segmentation.\n",
    "\n",
    "![Kernels](assets/kernel.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ce0f15-15f2-4408-a767-285623808f55",
   "metadata": {},
   "source": [
    "**How do Kernels Work?**\n",
    "\n",
    "Imagine you have a grayscale image, like a black and white photo. Each pixel in the image has a specific value that represents its intensity. The kernel is a small matrix of values that we slide across the image.\n",
    "\n",
    "To extract a feature, we take the kernel and place it on top of a part of the image. We then multiply each pixel in the kernel with the corresponding pixel in the image. We add up all these multiplications to get a single number, which represents the presence or absence of a certain feature in that part of the image.\n",
    "\n",
    "This process is repeated as the kernel slides across the entire image, capturing different parts and extracting features from each. Each kernel focuses on detecting a specific pattern or feature. It gradually builds a hierarchical understanding, starting with low-level features like edges, corners and progressing to higher-level features like shapes, textures or objects.\n",
    "\n",
    "**How do I define the values of a Kernel?**\n",
    "\n",
    "The values in the kernel itself can be initialized randomly or set using a predefined function at the beginning. But as the network is trained with data, these values get adjusted to optimize the network's performance in extracting meaningful features.\n",
    "\n",
    "### Strides\n",
    "\n",
    "In real life each image is made up of thousands of tiny dots called pixels. Each pixel represents a color. When we want to process this picture using a Convolutional Neural Network (CNN), it can take a lot of time and computing power because there are so many pixels to consider.\n",
    "\n",
    "To make things faster and more efficient, we can reduce the number of features, which are essentially the different patterns or characteristics that the CNN tries to learn from the image. One way to do this is by changing the `stride` parameter. The stride parameter determines how many pixels the CNN's analyzing window, called a kernel or filter, moves at a time.\n",
    "\n",
    "In the previous example, we took a single step while moving the kernel i.e. the stride parameter has value `1` in this case. If you set two strides=2 then you take two step right in a row-wise and two steps down in a column-wise movement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d5a102-8f93-4498-800f-bfd40dba2dd8",
   "metadata": {},
   "source": [
    "### Padding\n",
    "\n",
    "In a Convolutional Neural Network (CNN), padding is used to add extra pixels around the borders of an image before applying convolution. Padding can help preserve spatial information, mitigate the loss of border pixels during convolution, and control the size of the output feature map. The added pixels create a buffer zone around the original image, allowing the kernel to traverse the borders and capture features from the entire image.\n",
    "\n",
    "There are two common types of padding:\n",
    "\n",
    "- **Valid (No Padding):** In this case, no padding is applied, and the kernel is only applied to positions where it fully overlaps with the input image. As a result, the output feature map will have smaller dimensions compared to the input image.\n",
    "\n",
    "- **Same (Zero Padding):** The \"same\" padding ensures that the output feature map has the same spatial dimensions (width and height) as the input image. To achieve this, the number of padding pixels added to each dimension is determined by the kernel size. For an odd-sized kernel, the same number of pixels is added to both sides, while for an even-sized kernel, the extra pixel is added to the right/bottom side.\n",
    "\n",
    "![Padding](assets/padding.png)\n",
    "\n",
    "Padding helps in several ways:\n",
    "\n",
    "- Retaining spatial dimensions: Zero padding ensures that the spatial dimensions of the input and output remain the same. This can be useful to maintain information at the borders of the image.\n",
    "\n",
    "- Centering the kernel: Padding allows the kernel to be centered at the borders of the image, enabling it to process pixels in the border regions.\n",
    "\n",
    "- Controlling downsampling: Padding can prevent significant downsampling of the image by reducing the stride effect. It helps in preserving more spatial information throughout the convolutional layers.\n",
    "\n",
    "- Mitigating information loss: Padding helps in reducing the loss of important border pixels during convolution, which can be crucial in tasks like object detection or segmentation.\n",
    "\n",
    "The output size after applying kernels, strides and padding is given by:\n",
    "\n",
    "$Feature\\ Map\\ Size = \\frac{Input\\ Image\\ size - Kernel\\ size + 2 * Padding}{Stride} + 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a657450d-bc8f-480e-8374-5a6ea6d5ad89",
   "metadata": {},
   "source": [
    "## Activation Function\n",
    "\n",
    "After the convolution operation, an element-wise activation function (typically ReLU - Rectified Linear Unit) is applied to introduce non-linearity. This helps the network learn more complex representations.\n",
    "\n",
    "## Pooling Layer\n",
    "\n",
    "<video width=\"640\" height=\"360\" controls>\n",
    "  <source src=\"./assets/pooling.mp4\" type=\"video/mp4\">\n",
    "  Your browser does not support the video tag.\n",
    "</video>\n",
    "\n",
    "\n",
    "The video above is a very good demonstration of an insight that reducing the size of an image does not always result in information loss, as evidenced by the ability to identify a dog in a smaller version of the picture. Even a fraction (1/4th) of the original image is sufficient to deduce its content, obviating the need for processing the entire image in CNNs.\n",
    "\n",
    "Pooling plays a crucial role in this scenario, facilitating the reduction of the feature map's size.\n",
    "\n",
    "The primary purpose of pooling is to reduce the spatial dimensions of the feature map, maintaining relevant information while enhancing computational efficiency. Pooling achieves size reduction of the image (feature map) by employing a filter that extracts a single value from a block of features. The most common type of pooling is max pooling, which divides the input into small regions and outputs the maximum value within each region. This downsampling operation reduces the computational complexity and makes the network more robust to spatial translations and distortions.\n",
    "\n",
    "![Pooling Workflow](assets/pooling.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fbb56d-d3e4-4495-bba1-792679880cdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
